import sys
import subprocess
import os
import time
import argparse
import random
from pathlib import Path

# ==========================================
# 0. è‡ªå‹•åŒ–ç’°å¢ƒé…ç½®
# ==========================================
def setup_environment():
    required_packages = {
        "pandas": "pandas",
        "numpy": "numpy<2.0",
        "openpyxl": "openpyxl",
        "mlxtend": "mlxtend==0.23.1",
        "scipy": "scipy",
        "matplotlib": "matplotlib",
        "tabulate": "tabulate"
    }
    for lib_name, install_name in required_packages.items():
        try:
            __import__(lib_name)
        except ImportError:
            print(f"ğŸ“¦ æ­£åœ¨å®‰è£ {install_name}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", install_name])

setup_environment()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules
from mlxtend.preprocessing import TransactionEncoder
from tabulate import tabulate

RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# ==========================================
# 1. è³‡æ–™è®€å–èˆ‡å‰ç½®è™•ç†
# ==========================================
def preprocess_data(file_path: str, min_items_per_txn: int = 1):
    if not os.path.exists(file_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ '{file_path}'")
        print(f"   è«‹ç¢ºèªæ‚¨çš„æª”æ¡ˆè·¯å¾‘: {os.getcwd()}")
        return None

    print(f">>> [1] æ­£åœ¨è®€å–æ•¸æ“š: {file_path}")
    if file_path.endswith(".xlsx"):
        df = pd.read_excel(file_path, engine="openpyxl")
    else:
        df = pd.read_csv(file_path)

    # å‰”é™¤é€€è²¨èˆ‡è¨»éŠ· (QUANTITY <= 0)
    original_count = len(df)
    df = df[df["QUANTITY"] > 0]
    df["ITEM_ID"] = df["ITEM_ID"].astype(str)
    
    filtered_count = len(df)
    print(f"    - å·²å‰”é™¤ {original_count - filtered_count} ç­†é€€è²¨/ç•°å¸¸è³‡æ–™")

    # è½‰æ›ç‚ºäº¤æ˜“æ ¼å¼
    transactions = df.groupby("INVOICE_NO")["ITEM_ID"].apply(list).values.tolist()
    
    if min_items_per_txn > 1:
        transactions = [t for t in transactions if len(t) >= min_items_per_txn]

    print(f"    - æœ‰æ•ˆäº¤æ˜“ç­†æ•¸: {len(transactions)}")
    return transactions

# ==========================================
# 2. å»ºç«‹ç¨€ç–çŸ©é™£
# ==========================================
def build_sparse_onehot(transactions: list[list]) -> pd.DataFrame:
    print("\n>>> [2] æ­£åœ¨å»ºç«‹ Sparse One-Hot äº¤æ˜“çŸ©é™£...")
    te = TransactionEncoder()
    oht = te.fit(transactions).transform(transactions, sparse=True)
    df_onehot = pd.DataFrame.sparse.from_spmatrix(oht, columns=te.columns_)
    df_onehot.columns = [str(c) for c in df_onehot.columns]
    df_onehot = df_onehot.astype("Sparse[bool]")
    n_txn, n_items = df_onehot.shape
    print(f"    - äº¤æ˜“æ•¸: {n_txn:,}, å“é …æ•¸: {n_items:,}")
    return df_onehot

# ==========================================
# 3. é›™æ¼”ç®—æ³•æ¯”è¼ƒ
# ==========================================
def run_mining_algorithms(df_onehot: pd.DataFrame, min_support: float):
    start = time.time()
    frequent_ap = apriori(df_onehot, min_support=min_support, use_colnames=True)
    time_ap = time.time() - start

    start = time.time()
    frequent_fp = fpgrowth(df_onehot, min_support=min_support, use_colnames=True)
    time_fp = time.time() - start

    return {
        "frequent_fp": frequent_fp,
        "time_ap": time_ap,
        "time_fp": time_fp,
        "n_itemsets": len(frequent_fp)
    }

# ==========================================
# 4. å†—é¤˜è¦å‰‡å‰”é™¤
# ==========================================
def filter_redundant_rules(rules: pd.DataFrame) -> pd.DataFrame:
    if rules is None or rules.empty:
        return rules

    rules = rules.copy()
    rules["antecedents_set"] = rules["antecedents"].apply(frozenset)
    keep_mask = np.ones(len(rules), dtype=bool)
    
    for i, row in rules.iterrows():
        current_ant = row["antecedents_set"]
        current_con = row["consequents"]
        current_conf = row["confidence"]
        same_consequent_rules = rules[rules["consequents"] == current_con]
        
        for _, other_row in same_consequent_rules.iterrows():
            if i == _: continue
            other_ant = other_row["antecedents_set"]
            other_conf = other_row["confidence"]
            
            if other_ant.issubset(current_ant) and other_ant != current_ant:
                if other_conf >= current_conf:
                    keep_mask[i] = False
                    break
                    
    filtered = rules.loc[keep_mask].drop(columns=["antecedents_set"]).reset_index(drop=True)
    return filtered

# ==========================================
# 5. æ¨è–¦åŠŸèƒ½
# ==========================================
def recommend_products(purchased_items: list, rules_df: pd.DataFrame, verbose: bool = True):
    if rules_df is None or rules_df.empty:
        if verbose: print("    âš ï¸ ç›®å‰æ²’æœ‰è¦å‰‡ï¼Œç„¡æ³•é€²è¡Œæ¨è–¦ã€‚")
        return []

    purchased_set = set(map(str, purchased_items))
    recommendations = set()
    
    for _, rule in rules_df.iterrows():
        ant_raw = rule["antecedents"]
        con_raw = rule["consequents"]
        antecedents = set(ant_raw) if isinstance(ant_raw, (frozenset, set)) else set(ant_raw)
        consequents = set(con_raw) if isinstance(con_raw, (frozenset, set)) else set(con_raw)

        if antecedents.issubset(purchased_set):
            recommendations.update(consequents)

    recommendations -= purchased_set
    rec_list = sorted(list(recommendations))

    if verbose:
        if rec_list:
            print(f"æ¨è–¦æ‚¨è³¼è²·: {', '.join(rec_list)}")
        else:
            print(" ç„¡ç›¸ç¬¦è¦å‰‡ï¼Œæš«ç„¡æ¨è–¦ç”¢å“ã€‚")
        
    return rec_list

# ==========================================
# 6. å¯¦é©—èˆ‡åœ–è¡¨ç¹ªè£½
# ==========================================
def run_experiments(df_onehot, transactions):
    # æ”¹ç‚ºç›´æ¥å­˜æª”ï¼Œä¸ä½¿ç”¨ outputs è³‡æ–™å¤¾
    print("\n>>> [3] æ­£åœ¨é€²è¡Œåƒæ•¸å½±éŸ¿åˆ†æå¯¦é©— (åŒ…å«ç¹ªåœ–)...")
    
    supports = [0.001, 0.002, 0.005]
    confidences = [0.1, 0.3, 0.5]
    
    results = []
    test_baskets = [t for t in transactions if len(t) >= 2][:3]
    
    for s in supports:
        mining_res = run_mining_algorithms(df_onehot, min_support=s)
        frequent_itemsets = mining_res["frequent_fp"]
        
        for c in confidences:
            if frequent_itemsets.empty:
                rules_count = 0
                filtered_count = 0
                rec_count = 0
            else:
                rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=c)
                rules_count = len(rules)
                filtered_rules = filter_redundant_rules(rules)
                filtered_count = len(filtered_rules)
                
                unique_recs = set()
                for basket in test_baskets:
                    recs = recommend_products(basket, filtered_rules, verbose=False)
                    unique_recs.update(recs)
                rec_count = len(unique_recs)
            
            results.append({
                "Support": s,
                "Confidence": c,
                "Time(Apriori)": mining_res["time_ap"],
                "Time(FP-Growth)": mining_res["time_fp"],
                "Speedup": mining_res["time_ap"] / mining_res["time_fp"] if mining_res["time_fp"] > 0 else 0,
                "Raw Rules": rules_count,
                "Filtered Rules": filtered_count,
                "Rec Products": rec_count
            })
            
    results_df = pd.DataFrame(results)
    print("\nğŸ“Š åƒæ•¸å½±éŸ¿åˆ†æçµæœè¡¨:")
    print(tabulate(results_df, headers='keys', tablefmt='fancy_grid', floatfmt=".4f"))
    
    # å­˜æª” - ç›´æ¥å­˜ CSV
    results_df.to_csv("experiment_summary.csv", index=False, encoding="utf-8-sig")
    
    # ç¹ªåœ– - ç›´æ¥å­˜ png
    plt.figure(figsize=(10, 5))
    df_plot = results_df.drop_duplicates(subset=["Support"])
    plt.plot(df_plot["Support"], df_plot["Time(Apriori)"], marker='o', label='Apriori')
    plt.plot(df_plot["Support"], df_plot["Time(FP-Growth)"], marker='s', label='FP-Growth')
    plt.xlabel('Minimum Support')
    plt.ylabel('Time (seconds)')
    plt.title('Algorithm Runtime Comparison')
    plt.legend()
    plt.grid(True)
    plt.savefig("plot_runtime.png")
    
    plt.figure(figsize=(10, 5))
    df_plot_2 = results_df[results_df["Support"] == 0.001]
    plt.bar(df_plot_2["Confidence"].astype(str), df_plot_2["Filtered Rules"], color='skyblue')
    plt.xlabel('Minimum Confidence')
    plt.ylabel('Number of Rules')
    plt.title('Rules Count vs Confidence (Support=0.001)')
    plt.savefig("plot_rules.png")

    return results_df

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    file_name = "äº¤æ˜“è³‡æ–™é›†(2).xlsx" # æª”å
    
    # 1. å‰ç½®è™•ç†
    transactions = preprocess_data(file_name)
    
    if transactions:
        # 2. å»ºç«‹ç¨€ç–çŸ©é™£
        df_onehot = build_sparse_onehot(transactions)
        
        # 3. åŸ·è¡Œå®Œæ•´å¯¦é©—
        summary_df = run_experiments(df_onehot, transactions)
        
        # 4. ç”¢ç”Ÿæœ€çµ‚æœ€ä½³æ¨¡å‹
        print("\n>>> [4] ç”¢ç”Ÿæœ€çµ‚æ¨¡å‹ (Support=0.001, Conf=0.3)...")
        res = run_mining_algorithms(df_onehot, min_support=0.001)
        freq_items = res["frequent_fp"]
        rules = association_rules(freq_items, metric="confidence", min_threshold=0.3)
        final_rules = filter_redundant_rules(rules)
        
        # å­˜æª”è™•ç† (CSV + Excel)
        output_csv = "mining_results.csv"
        output_xlsx = "mining_results.xlsx"
        
        # è™•ç†è¼¸å‡ºæ ¼å¼
        save_rules = final_rules.copy()
        save_rules['antecedents'] = save_rules['antecedents'].apply(lambda x: ','.join(list(map(str, x))))
        save_rules['consequents'] = save_rules['consequents'].apply(lambda x: ','.join(list(map(str, x))))
        
        # å„²å­˜ CSV
        save_rules.to_csv(output_csv, index=False, encoding='utf-8-sig')
        print(f"    - [CSV] è¦å‰‡å·²å­˜æª”è‡³: {os.path.abspath(output_csv)}")
        
        # å„²å­˜ Excel (å› ç‚ºæ‚¨æåˆ°æƒ³è¦ Excel æª”)
        try:
            save_rules.to_excel(output_xlsx, index=False)
            print(f"    - [Excel] è¦å‰‡å·²å­˜æª”è‡³: {os.path.abspath(output_xlsx)}")
        except Exception as e:
            print(f"    [Excel å­˜æª”å¤±æ•— (å¯èƒ½ç¼ºå°‘ openpyxl): {e}")

        # 5. æ¨è–¦æ¸¬è©¦ (ä¿è­‰å‘½ä¸­ç‰ˆ)
        print("\n>>> [5] æ¨è–¦ç³»çµ±æ¸¬è©¦ï¼š")
        if not final_rules.empty:
            first_rule_antecedents = list(final_rules.iloc[0]['antecedents'])
            test_basket = list(first_rule_antecedents)
            print(f"    æ¨¡æ“¬é¡§å®¢è³¼è²·äº†: {test_basket}")
            recommend_products(test_basket, final_rules)
        else:
            print("æ²’æœ‰ç”¢ç”Ÿä»»ä½•è¦å‰‡ï¼Œç„¡æ³•é€²è¡Œæ¨è–¦æ¸¬è©¦ã€‚")
        
    print("\nâœ¨ å…¨éƒ¨å®Œæˆï¼æª”æ¡ˆå·²å„²å­˜æ–¼ç¨‹å¼æ‰€åœ¨ç›®éŒ„ã€‚")